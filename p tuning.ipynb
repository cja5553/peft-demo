{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6929450",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef2849bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    PeftType,\n",
    "    PromptEncoder,\n",
    "    PromptEncoderConfig\n",
    ")\n",
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10151f",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16a427e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Look forward to the detours because they bring...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GKN so lively as well, mad quick</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@siomo @NEWSTALK1010 20 says he gets reelected...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hope was an instinct only the reasoning human ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sure, ohio state is terrible, ohio is awful, e...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>@British_Airways #lost #bag #stillwaiting go h...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>If my luck the rest of Fall goes anything like...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>@Onision @Eugenia_Cooney annoyed by the good l...</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>Unbelievable takes 10 minutes to get through t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>Dad asked if I was too hungover to function to...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3613 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text    label  intensity\n",
       "0     Look forward to the detours because they bring...      joy      0.667\n",
       "1                      GKN so lively as well, mad quick      joy      0.320\n",
       "2     @siomo @NEWSTALK1010 20 says he gets reelected...     fear      0.271\n",
       "3     Hope was an instinct only the reasoning human ...     fear      0.521\n",
       "4     sure, ohio state is terrible, ohio is awful, e...     fear      0.500\n",
       "...                                                 ...      ...        ...\n",
       "3608  @British_Airways #lost #bag #stillwaiting go h...  sadness      0.542\n",
       "3609  If my luck the rest of Fall goes anything like...    anger      0.271\n",
       "3610  @Onision @Eugenia_Cooney annoyed by the good l...      joy      0.104\n",
       "3611  Unbelievable takes 10 minutes to get through t...    anger      0.604\n",
       "3612  Dad asked if I was too hungover to function to...  sadness      0.271\n",
       "\n",
       "[3613 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#load train data\n",
    "\n",
    "def get_file(path):\n",
    "    cols = ['id', 'text', 'label', 'intensity']\n",
    "    anger = pd.read_csv(path + 'anger.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "    fear = pd.read_csv(path + 'fear.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "    sad = pd.read_csv(path + 'sadness.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "    joy = pd.read_csv(path + 'joy.txt', header=None, sep='\\t', names=cols, index_col=0)\n",
    "    # Combine the DataFrames\n",
    "    combined_df = pd.concat([anger, fear, sad, joy])\n",
    "    # Shuffle the combined DataFrame\n",
    "    shuffled = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "    return shuffled\n",
    "\n",
    "train=get_file(\"train/\")\n",
    "val=get_file(\"Dev/\")\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1074e0c5",
   "metadata": {},
   "source": [
    "# Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c22eb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.dataframe.iloc[idx]['text']\n",
    "        label = self.dataframe.iloc[idx]['label']\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        inputs = self.tokenizer(sentence, truncation=True, max_length=None, return_tensors=\"pt\")\n",
    "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
    "\n",
    "        # Convert label to a numeric format if necessary\n",
    "        label_to_id = {'joy': 0, 'anger': 1, 'fear': 2, 'sadness': 3}\n",
    "        label_id = label_to_id[label]\n",
    "\n",
    "        inputs['labels'] = torch.tensor(label_id, dtype=torch.long)\n",
    "\n",
    "        return inputs\n",
    "\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding=\"longest\", return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a215a48",
   "metadata": {},
   "source": [
    "# Define the p tuning configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c92f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name_or_path = \"cardiffnlp/twitter-roberta-base-emotion\"\n",
    "peft_type = PeftType.P_TUNING\n",
    "device = \"cuda\"\n",
    "num_epochs = 4\n",
    "peft_config = PromptEncoderConfig(task_type=\"SEQ_CLS\", \n",
    "                                 num_virtual_tokens=20,\n",
    "                                 encoder_reparameterization_type=\"MLP\", #note the options are MLP, LSTM, or EMB\n",
    "                                 encoder_hidden_size=128) # hidden state size of MLP\n",
    "lr = 0.005\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf886607",
   "metadata": {},
   "source": [
    "# Define the pre-trained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cdbfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,padding_side='left')\n",
    "label_to_id = {'joy': 0, 'anger': 1, 'fear': 2, 'sadness': 3}\n",
    "# Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, num_labels=len(label_to_id))\n",
    "\n",
    "train_dataset = EmotionDataset(train, tokenizer)\n",
    "val_dataset = EmotionDataset(val, tokenizer)\n",
    "# Use the collate_fn in your DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, collate_fn=collate_fn, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=True, collate_fn=collate_fn, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8a843",
   "metadata": {},
   "source": [
    "### Print the model and peft details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ef081a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 823,044 || all params: 125,471,752 || trainable%: 0.6559595979818629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): PeftModelForSequenceClassification(\n",
       "    (base_model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 768)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-11): 12 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): ModuleDict(\n",
       "      (default): PromptEncoder(\n",
       "        (embedding): Embedding(20, 768)\n",
       "        (mlp_head): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=128, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "  )\n",
       "  (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6c72fd",
   "metadata": {},
   "source": [
    "# Define the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8fe54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(params=model.parameters(), lr=lr)\n",
    "\n",
    "# Instantiate scheduler\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0.06 * (len(train_dataloader) * num_epochs),\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e635a0",
   "metadata": {},
   "source": [
    "# Fine-tune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0531811f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/226 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 226/226 [00:06<00:00, 34.73it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 57.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6484149855907781\n",
      "F1 Score: 0.6357025344250339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 226/226 [00:05<00:00, 37.85it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 60.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.659942363112392\n",
      "F1 Score: 0.656506507834601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 226/226 [00:06<00:00, 37.59it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 61.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6829971181556196\n",
      "F1 Score: 0.6873436206358451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 226/226 [00:05<00:00, 38.67it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:00<00:00, 60.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.723342939481268\n",
      "F1 Score: 0.7226385163084428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "f1_metric = load('f1', config_name='multiclass', average='weighted')\n",
    "accuracy_metric = load('accuracy')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(tqdm(val_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        predictions, references = predictions, batch[\"labels\"]\n",
    "        accuracy_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        f1_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    # Compute final metric values\n",
    "    final_accuracy = accuracy_metric.compute()\n",
    "    final_f1 = f1_metric.compute(average='weighted')\n",
    "    print(f\"Accuracy: {final_accuracy['accuracy']}\")\n",
    "    print(f\"F1 Score: {final_f1['f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fb49e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
